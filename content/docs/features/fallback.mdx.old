---
title: Automatic Fallback
description: Ensure high availability with intelligent model fallback
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Steps, Step } from 'fumadocs-ui/components/steps';

# Automatic Fallback

Never let a model failure or rate limit interrupt your application. MegaLLM's automatic fallback ensures continuous operation by seamlessly switching between models when issues occur.

<Callout type="info">
  **Zero Downtime**: When one model fails, MegaLLM instantly routes to your backup models without any interruption to your users.
</Callout>

## How It Works

<Steps>
<Step>

### Primary Model Attempt

Your request first goes to your primary model of choice.

</Step>
<Step>

### Issue Detection

MegaLLM monitors for errors, timeouts, and rate limits in real-time.

</Step>
<Step>

### Automatic Switching

If an issue is detected, the request seamlessly routes to your fallback model.

</Step>
<Step>

### Response Delivery

You receive a successful response, regardless of which model ultimately handled it.

</Step>
</Steps>

## Configuration

### Basic Fallback

<Tabs items={["Python", "JavaScript", "cURL"]}>
<Tab value="Python">

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://ai.megallm.io/v1",
    api_key="your-api-key"
)

response = client.chat.completions.create(
    model="gpt-5",
    messages=[{"role": "user", "content": "Hello!"}],
    # Simple fallback chain
    fallback_models=["claude-opus-4", "gemini-2.5-pro", "gpt-4o"]
)

# Response includes which model was actually used
print(f"Response from: {response.model}")
print(response.choices[0].message.content)
```

</Tab>
<Tab value="JavaScript">

```javascript
const response = await fetch("https://ai.megallm.io/v1/chat/completions", {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${API_KEY}`,
    "Content-Type": "application/json"
  },
  body: JSON.stringify({
    model: "gpt-5",
    messages: [{ role: "user", content: "Hello!" }],
    fallback_models: ["claude-opus-4", "gemini-2.5-pro", "gpt-4o"]
  })
});

const data = await response.json();
console.log(`Response from: ${data.model}`);
```

</Tab>
<Tab value="cURL">

```bash
curl https://ai.megallm.io/v1/chat/completions \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5",
    "messages": [{"role": "user", "content": "Hello!"}],
    "fallback_models": ["claude-opus-4", "gemini-2.5-pro", "gpt-4o"]
  }'
```

</Tab>
</Tabs>

### Advanced Configuration

Configure detailed fallback behavior:

```python
response = client.chat.completions.create(
    model="gpt-5",
    messages=messages,
    fallback_config={
        "models": [
            {
                "model": "claude-opus-4",
                "conditions": ["rate_limit", "timeout"],
                "timeout": 30  # seconds
            },
            {
                "model": "gemini-2.5-pro",
                "conditions": ["error", "rate_limit"],
                "timeout": 20
            },
            {
                "model": "gpt-4o-mini",
                "conditions": ["all"],  # Fallback for any issue
                "timeout": 15
            }
        ],
        "retry_primary": True,  # Retry primary model first
        "max_retries": 2,       # Max retries per model
        "return_partial": False # Wait for complete response
    }
)
```

## Fallback Strategies

### 1. Performance-Based

Automatically switch to faster models when response time is critical:

```python
fallback_config={
    "strategy": "performance",
    "max_latency_ms": 2000,  # Switch if slower than 2 seconds
    "models": ["gpt-4o-mini", "gemini-2.0-flash", "gpt-3.5-turbo"]
}
```

### 2. Cost-Optimized

Start with cheaper models, escalate to premium only if needed:

```python
fallback_config={
    "strategy": "cost_optimized",
    "models": [
        {"model": "gpt-4o-mini", "max_attempts": 2},
        {"model": "claude-sonnet-3.5", "max_attempts": 1},
        {"model": "gpt-5", "max_attempts": 1}
    ]
}
```

### 3. Quality-First

Prioritize the best models, fall back to maintain availability:

```python
fallback_config={
    "strategy": "quality_first",
    "models": [
        "claude-opus-4.1",  # Best reasoning
        "gpt-5",            # Second best
        "gemini-2.5-pro",   # Good alternative
        "gpt-4o"            # Reliable fallback
    ]
}
```

### 4. Load Balancing

Distribute requests across multiple models:

```python
fallback_config={
    "strategy": "round_robin",
    "models": ["gpt-5", "claude-opus-4", "gemini-2.5-pro"],
    "sticky_sessions": True  # Keep conversations on same model
}
```

## Fallback Triggers

### Rate Limit Handling

```python
# Automatically handle rate limits
response = client.chat.completions.create(
    model="gpt-5",
    messages=messages,
    fallback_on_rate_limit=True,
    rate_limit_fallbacks=["claude-opus-4", "gemini-2.5-pro"]
)
```

### Error Recovery

```python
# Handle model errors gracefully
response = client.chat.completions.create(
    model="claude-opus-4",
    messages=messages,
    fallback_on_error=True,
    error_fallbacks={
        "5xx": ["gpt-5", "gemini-2.5-pro"],  # Server errors
        "4xx": ["gpt-4o"],                    # Client errors
        "timeout": ["gpt-4o-mini"]            # Timeout errors
    }
)
```

### Capacity Management

```python
# Switch models based on availability
response = client.chat.completions.create(
    model="gpt-5",
    messages=messages,
    fallback_on_capacity=True,
    capacity_threshold=0.8,  # Switch at 80% capacity
    capacity_fallbacks=["claude-opus-4", "gemini-2.5-pro"]
)
```

## Real-World Examples

### E-commerce Chatbot

Ensure your customer service never goes down:

```python
def handle_customer_query(query):
    return client.chat.completions.create(
        model="gpt-4o-mini",  # Fast, cost-effective primary
        messages=[
            {"role": "system", "content": "You are a helpful customer service agent."},
            {"role": "user", "content": query}
        ],
        fallback_models=[
            "gpt-3.5-turbo",    # Similar performance
            "claude-sonnet-3.5", # Good alternative
            "gpt-5-mini"        # Premium fallback
        ],
        fallback_on_rate_limit=True,
        fallback_on_error=True,
        max_retries=3
    )
```

### Code Generation

Maintain quality while ensuring availability:

```python
def generate_code(prompt):
    return client.chat.completions.create(
        model="grok-code-fast-1",  # Specialized for code
        messages=[
            {"role": "system", "content": "You are an expert programmer."},
            {"role": "user", "content": prompt}
        ],
        fallback_config={
            "models": [
                {"model": "gpt-5", "conditions": ["error", "rate_limit"]},
                {"model": "claude-sonnet-3.7", "conditions": ["all"]}
            ],
            "preserve_context": True,  # Maintain conversation context
            "quality_check": True      # Verify output quality
        }
    )
```

### Research Assistant

Complex reasoning with reliability:

```python
def research_query(question):
    return client.chat.completions.create(
        model="claude-opus-4.1",  # Best for complex reasoning
        messages=[
            {"role": "user", "content": f"Research question: {question}"}
        ],
        fallback_strategy={
            "primary_group": ["claude-opus-4.1", "gpt-5"],  # Try best models first
            "secondary_group": ["gemini-2.5-pro", "claude-sonnet-4"],  # Good alternatives
            "emergency": "gpt-4o",  # Always available
            "group_timeout": 30,  # 30 seconds per group
            "preserve_quality": True
        }
    )
```

## Monitoring & Analytics

Track your fallback usage:

```python
# Get fallback statistics
stats = client.get_fallback_stats(
    time_range="last_24h",
    group_by="model"
)

print(f"Primary success rate: {stats.primary_success_rate}%")
print(f"Fallback triggers: {stats.fallback_count}")
print(f"Most used fallback: {stats.top_fallback_model}")

# Set up alerts
client.set_fallback_alert(
    threshold=0.2,  # Alert if >20% requests use fallback
    email="ops@company.com"
)
```

## Best Practices

<Callout type="tip">
  **Model Similarity**: Choose fallback models with similar capabilities to ensure consistent output quality.
</Callout>

### 1. Tiered Fallbacks

```python
# Good: Similar models in each tier
fallback_tiers = [
    ["gpt-5", "claude-opus-4"],           # Tier 1: Premium
    ["gemini-2.5-pro", "claude-sonnet-4"], # Tier 2: Standard
    ["gpt-4o-mini", "gemini-flash"]       # Tier 3: Fast
]
```

### 2. Context Preservation

```python
# Maintain conversation context across fallbacks
fallback_config={
    "preserve_context": True,
    "context_window_adjustment": "automatic",
    "models": ["gpt-5", "claude-opus-4", "gemini-2.5-pro"]
}
```

### 3. Cost Control

```python
# Set spending limits for fallbacks
fallback_config={
    "max_cost_multiplier": 2.0,  # Don't exceed 2x primary cost
    "cost_aware_routing": True,
    "models": [
        {"model": "gpt-4o-mini", "cost": 0.75},
        {"model": "claude-sonnet-3.5", "cost": 18},
        {"model": "gpt-5", "cost": 11.25}
    ]
}
```

## Testing Fallbacks

Test your fallback configuration:

```python
# Test fallback behavior
test_result = client.test_fallback(
    primary_model="gpt-5",
    fallback_models=["claude-opus-4", "gemini-2.5-pro"],
    test_scenarios=["rate_limit", "timeout", "error"],
    dry_run=True
)

print(f"Fallback chain tested: {test_result.success}")
print(f"Average fallback time: {test_result.avg_fallback_ms}ms")
```

## Next Steps

- Configure [Model Comparison](/docs/features/comparison) for A/B testing
- Set up [Load Balancing](/docs/features/load-balancing) for optimal distribution
- View [Monitoring](/docs/features/monitoring) for tracking system health