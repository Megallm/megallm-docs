---
title: OpenAI API
description: Complete guide to OpenAI-compatible endpoints
---

import { Card, Cards } from 'fumadocs-ui/components/card';
import { Callout } from 'fumadocs-ui/components/callout';

# OpenAI API

MegaLLM provides full compatibility with OpenAI's API format, allowing you to use existing OpenAI SDKs and tools seamlessly.

<Callout type="info">
  **Base URL**: `https://ai.megallm.io/v1` for all OpenAI-compatible endpoints
</Callout>

## Available Endpoints

<Cards>
  <Card
    title="Chat Completions"
    description="Generate conversational responses with GPT models"
    href="/docs/openai/chat-completions"
  />
  <Card
    title="Streaming"
    description="Real-time streaming responses with Server-Sent Events"
    href="/docs/openai/streaming"
  />
  <Card
    title="Function Calling"
    description="Execute functions and tools with parallel support"
    href="/docs/openai/function-calling"
  />
  <Card
    title="Structured Output"
    description="Generate validated JSON with schema enforcement"
    href="/docs/openai/structured-output"
  />
  <Card
    title="Embeddings"
    description="Create vector embeddings for semantic search"
    href="/docs/openai/embeddings"
  />
  <Card
    title="Models"
    description="List and information about available models"
    href="/docs/openai/models"
  />
</Cards>

## Quick Example

```python
from openai import OpenAI

# Initialize client
client = OpenAI(
    base_url="https://ai.megallm.io/v1",
    api_key="your-api-key"
)

# Simple chat completion
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain quantum computing in simple terms."}
    ],
    temperature=0.7,
    max_tokens=150
)

print(response.choices[0].message.content)
```

## Supported Models

| Model | Context Window | Use Case |
|-------|---------------|-----------|
| `gpt-4` | 8,192 tokens | Complex reasoning, analysis |
| `gpt-4-32k` | 32,768 tokens | Long documents, extensive context |
| `gpt-4-turbo` | 128,000 tokens | Large-scale processing |
| `gpt-3.5-turbo` | 16,385 tokens | Fast, cost-effective responses |

## Features

### ðŸš€ Full Compatibility
Drop-in replacement for OpenAI API - use your existing code without changes.

### âš¡ High Performance
Fast response times with optimized infrastructure.

### ðŸ“Š Usage Tracking
Monitor your API usage and costs.

## SDK Support

MegaLLM works with all OpenAI-compatible SDKs:

- **Python**: `openai` official SDK
- **Node.js**: `openai` official SDK
- **Go**: `go-openai`
- **Rust**: `async-openai`
- **Java**: `openai-java`
- **C#**: `OpenAI-DotNet`

## Rate Limits

| Tier | Requests/min | Tokens/min |
|------|-------------|------------|
| Basic | 60 | 90,000 |
| Pro | 300 | 450,000 |
| Enterprise | Custom | Custom |

## Migration Guide

Migrating from OpenAI to MegaLLM is simple:

```python
# Before (OpenAI)
client = OpenAI(api_key="sk-...")

# After (MegaLLM)
client = OpenAI(
    base_url="https://ai.megallm.io/v1",
    api_key="your-api-key"
)
```

That's it! All your existing code continues to work.

## Error Handling

MegaLLM returns OpenAI-compatible error responses:

```json
{
  "error": {
    "message": "Invalid request parameter",
    "type": "invalid_request_error",
    "param": "temperature",
    "code": null
  }
}
```

<Callout type="tip">
  **Pro Tip**: Enable debug mode with `X-Debug: true` header to get detailed error information during development.
</Callout>

## Next Steps

- Explore [Chat Completions](/docs/openai/chat-completions) for conversational AI
- Learn about [Function Calling](/docs/openai/function-calling) for tool integration
- Implement [Streaming](/docs/openai/streaming) for real-time responses